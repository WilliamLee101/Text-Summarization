{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 15:40:36.922212: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding, TimeDistributed\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Attention, Concatenate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from numpy.random import seed\n",
    "tensorflow.random.set_seed(2022)\n",
    "seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a word contraction dictionary I got from: https://www.kaggle.com/code/singhabhiiitkgp/text-summarization-using-lstm, I use this to help me remove contracted words and replace them with common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "data = get_data(\"/Users/williamlee/Desktop/CS505/Final Project/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[\"Text\"]\n",
    "y_train = data[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/williamlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #determine outliers by length\n",
    "# #I am using mean and standard deviatoion to determine which sentences are outliers\n",
    "\n",
    "# length_text = []\n",
    "# length_summary = []\n",
    "# for x in range(len(X_train)):\n",
    "#     length_text.append(len(X_train[x].split()))\n",
    "#     length_summary.append(len(y_train[x].split()))\n",
    "    \n",
    "# def cal_mean_and_std(test_list):\n",
    "#     mean = sum(test_list) / len(test_list)\n",
    "#     variance = sum([((x - mean) ** 2) for x in test_list]) / len(test_list)\n",
    "#     res = variance ** 0.5\n",
    "#     return mean, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_mean, X_std = cal_mean_and_std(length_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_mean, y_std = cal_mean_and_std(length_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_texts = []\n",
    "# new_summaries = []\n",
    "# for i in range(len(X_train)):\n",
    "#     if len(X_train[i].split())<=X_mean+3*X_std and len(y_train[i].split())<=y_mean+3*y_std:\n",
    "#         new_texts.append(X_train[i])\n",
    "#         new_summaries.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We have to add some words to indicate the start and end of the summaries\n",
    "# #I am not sure why people use sostok and eostok, but I notice that almost everyone use these, so \n",
    "# #I am going to use the same\n",
    "\n",
    "# new_df = pd.DataFrame({'text': new_texts,'summary': new_summaries})\n",
    "# new_df['summary'] = new_df['summary'].apply(lambda x: 'sostok ' + x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split the data into training data and testing data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test=train_test_split(new_df['text'],new_df['summary'],test_size=0.1,random_state=2022,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #here, we are counting the number of words that appears less than 5 times\n",
    "# #Then, we are going to use that count to initialize a new tokenizer, which we neglect words that appears less than 5 times\n",
    "\n",
    "# def tokenize(X_train, X_test):\n",
    "#     x_tokenizer = Tokenizer() \n",
    "#     x_tokenizer.fit_on_texts(list(X_train))\n",
    "    \n",
    "#     val = 5\n",
    "#     count = 0\n",
    "#     total_count = 0\n",
    "\n",
    "#     for key, value in x_tokenizer.word_counts.items():\n",
    "#         total_count += 1\n",
    "#         if value < val:\n",
    "#             count += 1\n",
    "\n",
    "#     count = total_count - total_count\n",
    "#     tokenizer = Tokenizer(num_words = count)\n",
    "#     inp = list(X_train)\n",
    "#     tokenizer.fit_on_texts(inp)\n",
    "    \n",
    "#     max_len = int(X_mean+3*X_std)\n",
    "#     train_seq = tokenizer.texts_to_sequences(X_train) \n",
    "#     train = pad_sequences(train_seq,  maxlen=max_len, padding='post')\n",
    "    \n",
    "#     test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "#     test = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
    "    \n",
    "#     voc =  tokenizer.num_words + 1\n",
    "    \n",
    "#     return train, test, voc, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, X_voc, X_tokenizer = tokenize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test, y_voc, y_tokenizer = tokenize(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #we are going to delete the samples with empty summaries\n",
    "\n",
    "# def delete_empty(x, y):\n",
    "#     ind=[]\n",
    "#     for i in range(len(y)):\n",
    "#         count=0\n",
    "#         for j in y[i]:\n",
    "#             if j!=0:\n",
    "#                 count+=1\n",
    "#         if count==2:\n",
    "#             ind.append(i)\n",
    "    \n",
    "# #     y_set = []\n",
    "# #     x_set = []\n",
    "# #     for i in range(len(x)):\n",
    "# #         if i not in ind:\n",
    "# #             x_set.append(x[i])\n",
    "# #             y_set.append(y[i])\n",
    "            \n",
    "# #     x_set = np.array(x_set)\n",
    "# #     y_set = np.array(y_set)\n",
    "    \n",
    "#     x_set=np.delete(x,ind, axis=0)\n",
    "#     y_set=np.delete(y,ind, axis=0)\n",
    "    \n",
    "    \n",
    "#     return x_set, y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = delete_empty(X_train, y_train)\n",
    "# X_test, y_test = delete_empty(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our LSTM model\n",
    "\n",
    "# n_tags = 5022\n",
    "\n",
    "# input = Input(shape=(int(X_mean+3*X_std),))\n",
    "# model = Embedding(input_dim=17512, output_dim=5000, trainable=True)(input)  \n",
    "# model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2))(model)\n",
    "# model = TimeDistributed(Dense(5022, activation=\"softmax\"))(model)\n",
    "# crf = CRF(n_tags+1)\n",
    "# out = crf(model)\n",
    "\n",
    "# model = Model(input, out)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got this inspiration from the following websites and articles:<br>\n",
    "https://www.researchgate.net/publication/348531563_Text_Summarization_Using_Text_Frequency_Ranking_Sentence_Prediction<br>\n",
    "https://aip.scitation.org/doi/pdf/10.1063/5.0037283<br>\n",
    "https://medium.com/@ashins1997/text-summarization-e702168569f6<br>\n",
    "https://ijisrt.com/wp-content/uploads/2019/07/IJISRT19JU589.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizarion_freq(text, percentage):\n",
    "    #perform some tokenization, from assginments\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "#     print()\n",
    "#     print(tokens)\n",
    "#     print()\n",
    "    for index in range(len(tokens)):\n",
    "        if tokens[index] in contraction_mapping:\n",
    "            tokens[index]=contraction_mapping[tokens[index]]\n",
    "    text = ' '.join(tokens)\n",
    "#     print(text)\n",
    "    doc= nlp(text)\n",
    "            \n",
    "    \n",
    "    #Next we are going to find the frequencies of tokens\n",
    "    freq={}\n",
    "    for word in doc:\n",
    "        if word.text not in list(STOP_WORDS) and word.text not in punctuation:\n",
    "            if word.text not in freq.keys():\n",
    "                freq[word.text]=1\n",
    "            else:\n",
    "                freq[word.text]+=1\n",
    "                \n",
    "    #get the maximum frequency\n",
    "    max_freq = max(list(freq.values()))\n",
    "    \n",
    "    for key in freq.keys():\n",
    "        freq[key] = freq[key]/max_freq\n",
    "    \n",
    "    #we are going to compute the score, frequency sum of each sentence\n",
    "    sent_tokens = [sentence for sentence in doc.sents]\n",
    "    scores = {}\n",
    "    for sent in sent_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in freq:\n",
    "                if sent not in scores:\n",
    "                    scores[sent] = freq[word.text.lower()]\n",
    "                else:\n",
    "                    scores[sent] += freq[word.text.lower()]\n",
    "    \n",
    "    # We are going to pick the sentences with the top priorities with given percentage\n",
    "    select_len = int(len(sent_tokens)*percentage)\n",
    "    summary = nlargest(select_len, scores, key=scores.get)\n",
    "#     print()\n",
    "#     print(scores)\n",
    "#     print()\n",
    "#     print(summary)\n",
    "#     print()\n",
    "    summary = [w.text for w in summary]\n",
    "#     print()\n",
    "#     print(summary)\n",
    "#     print()\n",
    "    summary = ''.join(summary)\n",
    "    summary = summary.replace(\"\\n\", \"\")\n",
    "    summary = summary.replace(\" '\", \"'\")\n",
    "    summary = summary.replace(\" ,\", \",\")\n",
    "    summary = summary.replace(\" .\", \".\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test the summarizer on a long text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"/Users/williamlee/Desktop/CS505/Final Project/dataset/DUC2004/DUC2004_Summarization_Documents/duc2004_testdata/tasks1and2/duc2004_tasks1and2_docs/docs/1/D1.txt\")\n",
    "text = my_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCambodian leader Hun Sen on Friday rejected opposition parties' demands \\nfor talks outside the country, accusing them of trying to ``internationalize'' \\nthe political crisis. Government and opposition parties have asked \\nKing Norodom Sihanouk to host a summit meeting after a series of post-election \\nnegotiations between the two opposition groups and Hun Sen's party \\nto form a new government failed. Opposition leaders Prince Norodom \\nRanariddh and Sam Rainsy, citing Hun Sen's threats to arrest opposition \\nfigures after two alleged attempts on his life, said they could not \\nnegotiate freely in Cambodia and called for talks at Sihanouk's residence \\nin Beijing. Hun Sen, however, rejected that. ``I would like to make \\nit clear that all meetings related to Cambodian affairs must be conducted \\nin the Kingdom of Cambodia,'' Hun Sen told reporters after a Cabinet \\nmeeting on Friday. ``No-one should internationalize Cambodian affairs. \\nIt is detrimental to the sovereignty of Cambodia,'' he said. Hun Sen's \\nCambodian People's Party won 64 of the 122 parliamentary seats in \\nJuly's elections, short of the two-thirds majority needed to form \\na government on its own. Ranariddh and Sam Rainsy have charged that \\nHun Sen's victory in the elections was achieved through widespread \\nfraud. They have demanded a thorough investigation into their election \\ncomplaints as a precondition for their cooperation in getting the \\nnational assembly moving and a new government formed. Hun Sen said \\non Friday that the opposition concerns over their safety in the country \\nwas ``just an excuse for them to stay abroad.'' Both Ranariddh and \\nSam Rainsy have been outside the country since parliament was ceremonially \\nopened on Sep. 24. Sam Rainsy and a number of opposition figures have \\nbeen under court investigation for a grenade attack on Hun Sen's Phnom \\nPenh residence on Sep. 7. Hun Sen was not home at the time of the \\nattack, which was followed by a police crackdown on demonstrators \\ncontesting Hun Sen's election victory. The Sam Rainsy Party, in a \\nstatement released Friday, accused Hun Sen of being ``unwilling to \\nmake any compromise'' on negotiations to break the deadlock. ``A meeting \\noutside Cambodia, as suggested by the opposition, could place all \\nparties on more equal footing,'' said the statement. ``But the ruling \\nparty refuses to negotiate unless it is able to threaten its negotiating \\npartners with arrest or worse.'' \\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opposition leaders Prince Norodom  Ranariddh and Sam Rainsy, citing Hun Sen's threats to arrest opposition  figures after two alleged attempts on his life, said they could not  negotiate freely in Cambodia and called for talks at Sihanouk's residence  in Beijing.Government and opposition parties have asked  King Norodom Sihanouk to host a summit meeting after a series of post - election  negotiations between the two opposition groups and Hun Sen's party  to form a new government failed. Cambodian leader Hun Sen on Friday rejected opposition parties' demands  for talks outside the country, accusing them of trying to ` ` internationalize''  the political crisis.\n"
     ]
    }
   ],
   "source": [
    "summary = summarizarion_freq(text, 0.2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prospects were dim for resolution of the political crisis in Cambodia in October 1998.\n",
      "Prime Minister Hun Sen insisted that talks take place in Cambodia while opposition leaders Ranariddh and Sam Rainsy, fearing arrest at home, wanted them abroad.\n",
      "King Sihanouk declined to chair talks in either place.\n",
      "A U.S. House resolution criticized Hun Sen's regime while the opposition tried to cut off his access to loans.\n",
      "But in November the King announced a coalition government with Hun Sen heading the executive and Ranariddh leading the parliament.\n",
      "Left out, Sam Rainsy sought the King's assurance of Hun Sen's promise of safety and freedom for all politicians.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_summary = open(\"/Users/williamlee/Desktop/CS505/Final Project/dataset/DUC2004/reference/Task1_reference1.txt\").read()\n",
    "print(sample_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this frequency summarizer handles short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product looks more like a stew than a processed meat and it smells better.\n"
     ]
    }
   ],
   "source": [
    "print(summarizarion_freq(X_train[0], 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Quality Dog Food\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well T5 Transformer summarizes long and short texts.<br>\n",
    "Reference: https://huggingface.co/transformers/v3.0.2/model_doc/t5.html<br>\n",
    "Result: T5 does a bad work for both long and shor texts, they miss the important sentences. In the long text, there is no mention of who and where. For the short text, the thesis is that the dog food has good qulity, but there is no mention of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamlee/opt/anaconda3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/williamlee/opt/anaconda3/lib/python3.8/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> government and opposition parties have asked the opposition to host a summit. the two opposition groups\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "concat_text = \"summarize:\" + text\n",
    "input=tokenizer.encode(concat_text, return_tensors='pt', max_length=80)\n",
    "summary = my_model.generate(input)\n",
    "summary_T5 = tokenizer.decode(summary[0])\n",
    "print(summary_T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> canned dog food products have been found all over the world. i have bought several of\n"
     ]
    }
   ],
   "source": [
    "my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "concat_text = \"summarize:\" + X_train[0]\n",
    "input=tokenizer.encode(concat_text, return_tensors='pt', max_length=20)\n",
    "summary = my_model.generate(input)\n",
    "summary_T5 = tokenizer.decode(summary[0])\n",
    "print(summary_T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well BART Transformer summarizes long and short texts. <br>\n",
    "Reference: https://huggingface.co/transformers/v2.11.0/model_doc/bart.html <br>\n",
    "Result: BART does a good work for the long text, but it wasn't able to capture important information such as locations.\n",
    "    Fro the short text, BART does a really bad work, as it just returns the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamlee/opt/anaconda3/lib/python3.8/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 142 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambodian leader Hun Sen rejects opposition parties' demands for talks outside the country. Government and opposition parties have asked King Norodom Sihanouk to host a summit meeting. Opposition leaders cite Hun Sen's threats to arrest opposition figures after two alleged attempts on his life. Ranariddh and Sam Rainsy have charged that Hun Sen's victory was achieved through widespread fraud.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "inputs = tokenizer.batch_encode_plus([text],return_tensors='pt')\n",
    "summary = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "summary_BART = tokenizer.decode(summary[0], skip_special_tokens=True)\n",
    "print(summary_BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "inputs = tokenizer.batch_encode_plus([X_train[0]],return_tensors='pt')\n",
    "summary = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "summary_BART = tokenizer.decode(summary[0], skip_special_tokens=True)\n",
    "print(summary_BART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well GPT2 Transformer summarizes long and short texts. <br>\n",
    "Reference: https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html <br>\n",
    "Result: GPT2 does a terrible works for long texts, GPT2 doens't really capture all the important things form the original text. For the short text, it does a pretty good work at summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 120, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cambodian leader Hun Sen on Friday rejected opposition parties' demands \n",
      "for talks outside the country, accusing them of trying to ``internationalize'' \n",
      "the political crisis. Government and opposition parties have asked \n",
      "King Norodom Sihanouk to host a summit meeting after a series of post-election \n",
      "negotiations between the two opposition groups and Hun Sen's party \n",
      "to form a new government failed. Opposition leaders Prince Norodom \n",
      "Ranariddh and Sam Rainsy, citing Hun Sen's threats to arrest opposition \n",
      "figures\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "inputs=tokenizer.batch_encode_plus([text],return_tensors='pt',max_length=120)\n",
    "summary=model.generate(inputs['input_ids'],early_stopping=True)\n",
    "summary_GPT=tokenizer.decode(summary[0],skip_special_tokens=True)\n",
    "print(summary_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have bought several of the Vitality canned dog food products and have been very pleased with the quality\n"
     ]
    }
   ],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "inputs=tokenizer.batch_encode_plus([X_train[0]],return_tensors='pt',max_length=10)\n",
    "summary=model.generate(inputs['input_ids'],early_stopping=True)\n",
    "summary_GPT=tokenizer.decode(summary[0],skip_special_tokens=True)\n",
    "print(summary_GPT)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
